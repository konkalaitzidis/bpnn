{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "298e493d-7c15-4116-970f-7a6de5f5d8e9",
   "metadata": {
    "tags": []
   },
   "source": [
<<<<<<< HEAD:src/V3/BPNN_V3.ipynb
    "## Setup"
=======
    "# Behaviour Prediction Neural Network Toolkit (BPNN)"
>>>>>>> revamp-release:bpnn/bpnn.ipynb
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD:src/V3/BPNN_V3.ipynb
   "id": "a9217b3e",
   "metadata": {},
   "source": [
    "Install dependencies by running the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dcfe1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install tensorflow\n",
    "# %pip install pandas"
=======
   "id": "af316d1f",
   "metadata": {},
   "source": [
    "Execute the steps of the BPNN pipeline in a step-by-step fashion. "
>>>>>>> revamp-release:bpnn/bpnn.ipynb
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864e9fa8-09d6-49cf-a127-1aeb7cccb5e6",
   "metadata": {
    "tags": []
   },
   "source": [
<<<<<<< HEAD:src/V3/BPNN_V3.ipynb
    "### Step 1: Are you using a GPU?"
=======
    "### Step 1: Determine computational resources (GPU or CPU?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91bd8c1",
   "metadata": {},
   "source": [
    "⚠️ Currently this pipeline is undergoing modification to be dynamically executed on either CPU or GPU resources. ⚠️"
>>>>>>> revamp-release:bpnn/bpnn.ipynb
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32c44e22-9669-4ced-ae27-8e58fdf46e77",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD:src/V3/BPNN_V3.ipynb
      "2024-10-09 23:21:00.987525: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
=======
      "2024-11-14 13:16:06.558327: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
>>>>>>> revamp-release:bpnn/bpnn.ipynb
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD:src/V3/BPNN_V3.ipynb
      "Num GPUs Available:  0\n"
=======
      "\n",
      "No GPU available\n"
>>>>>>> revamp-release:bpnn/bpnn.ipynb
     ]
    }
   ],
   "source": [
    "# Run this cell to determine whether you have a GPU available\n",
    "GPU_usage = False\n",
    "import tensorflow.compat.v1 as tf\n",
    "if GPU_usage == False:\n",
    "    print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "    tf.ones(1) + tf.ones(1)\n",
    "else:\n",
    "    print(\"\\nNo GPU available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd8c016-b8a5-4fe0-a23b-855a1541356a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 2: Import libraries and packages"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:src/V3/BPNN_V3.ipynb
   "execution_count": 3,
   "id": "cc9f33ac-2298-44bc-addc-fc0b862c345c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m \u001b[38;5;66;03m# for working with arrays and matrices\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m \u001b[38;5;66;03m# for data manipulation and analysis\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m \u001b[38;5;66;03m# for data visualization\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m \u001b[38;5;66;03m# for data visualization\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# # Secondary packages\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
=======
   "execution_count": 2,
   "id": "fcbef033",
   "metadata": {},
   "outputs": [],
>>>>>>> revamp-release:bpnn/bpnn.ipynb
   "source": [
    "# Main packages\n",
    "\n",
    "import numpy as np # for working with arrays and matrices\n",
    "import pandas as pd # for data manipulation and analysis\n",
    "#%pip install matplotlib\n",
    "import matplotlib.pyplot as plt # for data visualization\n",
    "# import seaborn as sns # for data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65adc56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (2024.10.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Other packages\n",
    "\n",
    "# import time # for time-related functions\n",
    "# import random # for random number generation\n",
    "# import cv2 # for computer vision and image processing tasks\n",
    "# import datetime # for saving date and time information\n",
    "# import csv # for loading csv files\n",
    "# import h5py # for working with HDF5 (Hierarchical Data Format) files\n",
    "# import boto3 # for working with Amazon Web Services (AWS)\n",
    "# from pynwb import NWBHDF5IO # for working with Neurodata Without Border (NWB) files\n",
    "%pip install fsspec\n",
    "import fsspec # \"File System Specifications\" and is a Python library that provides a unified interface for accessing various file systems and storage backends.\n",
    "# from fsspec.implementations.cached import CachingFileSystem # library used for working with various file systems in Python.\n",
    "# import requests #  simplifies making HTTP requests.\n",
    "# import aiohttp # libraries which are used for making HTTP requests in Python.\n",
    "import os # OS module provides various operating system-related functions to the code\n",
    "# import pickle # allows you to convert Python objects into a binary representation (serialization) that can be stored or transmitted, and later restore (deserialization) those objects back into Python.\n",
    "# import re # The re module is a standard library in Python that provides support for regular expressions, which are powerful tools for pattern matching and manipulation of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efb91e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML/DL-related packages\n",
    "\n",
    "# used for splitting data into training and testing sets in Python.\n",
    "# from sklearn.model_selection import train_test_split \n",
    "# from tensorflow.keras import datasets, layers, models\n",
    "# from tensorflow.keras.utils import plot_model\n",
    "# import imgaug.augmenters as iaa\n",
    "# from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "728bc181",
   "metadata": {
    "tags": [
     "TODO"
    ]
   },
   "outputs": [],
   "source": [
    "# Importing functions\n",
    "\n",
    "# import importlib\n",
    "from src.load_calcium_video import load_video_data, load_one_video\n",
    "# from pixel_values_normalization import normalize_video\n",
    "# from align_behavior_to_calcium import align_files_old_labels, align_files_new_labels\n",
    "# from class_balance import check_class_imbalance_old, check_class_imbalance_new, check_class_imbalance_old_merged\n",
    "# from model_architecture import construct_model\n",
    "# from preprocessing_model import model_preprocessing\n",
    "# from run_model import model_execution\n",
    "# from save_model_info import save_training_info\n",
    "# from plots import plot_first_frames, plot_random_frames, plot_image_pixel_values\n",
    "# from run_BPNN import run\n",
    "# import run_k_fold_model\n",
    "# import keras_tuner\n",
    "# from urllib.parse import urlparse\n",
    "# from ann_visualizer.visualize import ann_viz\n",
    "# import sys\n",
    "# # sys.path.append('/home/dmc/Desktop/kostas/direct-Behavior-prediction-from-miniscope-calcium-imaging-using-convolutional-neural-networks/src/V3')\n",
    "# from nwb_data_generator import NWBDataGeneratorTime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb18f9a-1d70-46af-9d35-09299f269874",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 3: Set and store experiment details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04aa8ec7-535f-4071-932c-741a0b5d4c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What will you be executing today?\n",
    "comment =  \"revamping the BPNN model\"\n",
    "\n",
    "# Give this experiment an ID number\n",
    "experiment_ID = 'test-1'\n",
    "\n",
    "# Which animal data files are you using?\n",
    "data_file = 'Animal_Doe'\n",
    "\n",
    "# What is the experiment's name?\n",
    "experiment_name = str(data_file)+\"_\"+str(experiment_ID)\n",
    "\n",
    "# What is the train-test split strategy?\n",
    "train_test_split_strategy = \"k-fold\"\n",
    "\n",
    "# What is the version of the BPNN model?\n",
    "name = 'BPNN_revamp'\n",
    "\n",
    "# What is the BPNN's version?\n",
    "# model_version = str(name)+'_1'\n",
    "\n",
    "# Which labels are you using? Old or New?\n",
    "labels_type_new = True # 'old'\n",
    "\n",
    "# Will you merge labels or not?\n",
    "merge_labels = False\n",
    "\n",
    "# Would you like to shuffle the labels?\n",
    "# shuffle = True\n",
    "\n",
    "# Are you analysing multiple videos or only one?\n",
    "multiple_videos = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "541f27e2-30bf-432a-8949-2afb2aa80708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'experiment_ID' (str)\n",
      "Stored 'data_file' (str)\n",
      "Stored 'experiment_name' (str)\n",
      "Stored 'name' (str)\n",
      "Stored 'labels_type_new' (bool)\n",
      "Stored 'merge_labels' (bool)\n",
      "Stored 'multiple_videos' (bool)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/konstantinoskalaitzidis/Library/Python/3.9/lib/python/site-packages/IPython/extensions/storemagic.py:229: UserWarning: using autorestore/experiment_ID requires you to install the `pickleshare` library.\n",
      "  db[ 'autorestore/' + arg ] = obj\n",
      "/Users/konstantinoskalaitzidis/Library/Python/3.9/lib/python/site-packages/IPython/extensions/storemagic.py:229: UserWarning: using autorestore/data_file requires you to install the `pickleshare` library.\n",
      "  db[ 'autorestore/' + arg ] = obj\n",
      "/Users/konstantinoskalaitzidis/Library/Python/3.9/lib/python/site-packages/IPython/extensions/storemagic.py:229: UserWarning: using autorestore/experiment_name requires you to install the `pickleshare` library.\n",
      "  db[ 'autorestore/' + arg ] = obj\n",
      "/Users/konstantinoskalaitzidis/Library/Python/3.9/lib/python/site-packages/IPython/extensions/storemagic.py:229: UserWarning: using autorestore/name requires you to install the `pickleshare` library.\n",
      "  db[ 'autorestore/' + arg ] = obj\n",
      "/Users/konstantinoskalaitzidis/Library/Python/3.9/lib/python/site-packages/IPython/extensions/storemagic.py:229: UserWarning: using autorestore/labels_type_new requires you to install the `pickleshare` library.\n",
      "  db[ 'autorestore/' + arg ] = obj\n",
      "/Users/konstantinoskalaitzidis/Library/Python/3.9/lib/python/site-packages/IPython/extensions/storemagic.py:229: UserWarning: using autorestore/merge_labels requires you to install the `pickleshare` library.\n",
      "  db[ 'autorestore/' + arg ] = obj\n",
      "/Users/konstantinoskalaitzidis/Library/Python/3.9/lib/python/site-packages/IPython/extensions/storemagic.py:229: UserWarning: using autorestore/multiple_videos requires you to install the `pickleshare` library.\n",
      "  db[ 'autorestore/' + arg ] = obj\n"
     ]
    }
   ],
   "source": [
    "# store the experiment variables:\n",
    "%store experiment_ID\n",
    "%store data_file\n",
    "%store experiment_name\n",
    "# %store train_test_split_strategy\n",
    "%store name\n",
    "# %store model_version\n",
    "%store labels_type_new\n",
    "%store merge_labels\n",
    "%store multiple_videos\n",
    "# %store shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ce5177-ca15-4031-87bb-1cc502878fd8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 4: Create a directory for the output files (update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "128440b6-9313-4558-9fd8-33fbd4a247ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'output_dir' (str)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/konstantinoskalaitzidis/Library/Python/3.9/lib/python/site-packages/IPython/extensions/storemagic.py:229: UserWarning: using autorestore/output_dir requires you to install the `pickleshare` library.\n",
      "  db[ 'autorestore/' + arg ] = obj\n"
     ]
    }
   ],
   "source": [
    "output_dir = str(experiment_ID)+'_output'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "    os.mkdir(os.path.join(output_dir, \"balance\"))\n",
    "    os.mkdir(os.path.join(output_dir, \"accuracy\"))\n",
    "    os.mkdir(os.path.join(output_dir, \"loss\"))\n",
    "    os.mkdir(os.path.join(output_dir, \"cm\"))\n",
    "    os.mkdir(os.path.join(output_dir, \"architecture\"))\n",
    "    os.mkdir(os.path.join(output_dir, \"pickles\"))\n",
    "    # os.mkdir(os.path.join(output_dir, \"results\"))\n",
    "    \n",
    "%store output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded2c471-893f-4aba-835a-a4a8522f0a0f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Step 5: Load the Calcium Videos that will be analysed (update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d120ca4-00ce-4a0d-876e-cd85d207e818",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_name_list = []\n",
    "video_data_list = []\n",
    "\n",
    "if multiple_videos == True:\n",
    "    \n",
    "    # paths to videos\n",
    "    video_paths = [\"/home/dmc/Desktop/kostas/direct-Behavior-prediction-from-miniscope-calcium-imaging-using-convolutional-neural-networks/data/20211025_184906_animal3learnday8.nwb\", \n",
    "                   \"/home/dmc/Desktop/kostas/direct-Behavior-prediction-from-miniscope-calcium-imaging-using-convolutional-neural-networks/data/20211026_142935_animal3learnday9.nwb\", \n",
    "                   \"/home/dmc/Desktop/kostas/direct-Behavior-prediction-from-miniscope-calcium-imaging-using-convolutional-neural-networks/data/20211027_165052_animal3learnday10.nwb\"]\n",
    "    \n",
    "    # load the CSV file with the FOV information\n",
    "    fov_info = pd.read_csv('/home/dmc/Desktop/kostas/direct-Behavior-prediction-from-miniscope-calcium-imaging-using-convolutional-neural-networks/src/V3/aligned_videos_animal3.csv')\n",
    "    images = load_video_data(video_paths, fov_info, video_name_list, video_data_list)\n",
    "\n",
    "    \n",
    "else:\n",
    "    # change the path if you're using another video other than animal3learnday11\n",
    "    # video_path = [\"/home/dmc/Desktop/kostas/direct-Behavior-prediction-from-miniscope-calcium-imaging-using-convolutional-neural-networks/data/20211028_174510_animal2learnday11.nwb\"]    \n",
    "    video_path = [\"/home/dmc/Desktop/kostas/direct-Behavior-prediction-from-miniscope-calcium-imaging-using-convolutional-neural-networks/data/20211028_181307_animal3learnday11.nwb\"]\n",
    "    images = load_one_video(video_path, video_name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3d9dc2-38ac-495a-b8f8-fa2ee5bb584b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check if the video has been loaded correctly. It should be an array of pixel values\n",
    "images[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5e2e43-31a0-454a-8264-5227aef46851",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Determine the size of the calcium video dataset\n",
    "num_of_frames = images.shape[0]\n",
    "img_height = images.shape[1]\n",
    "img_width = images.shape[2]\n",
    "print(\"The number of frames in the calcium imaging video is \", num_of_frames, \"and the frame dimensions (height x width) are: \", img_height, \"X\", img_width)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc2ab01-9899-4102-9cc0-3886660ee094",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Step 6: Load Behaviour Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b9476a-23e9-4bf7-ab3c-ae26bf707d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set this depending on how many videos you're reading\n",
    "num_of_videos = 1\n",
    "\n",
    "\n",
    "if multiple_videos == False:\n",
    "    \n",
    "    # bonsai_paths = [\"/home/dmc/Desktop/kostas/direct-Behavior-prediction-from-miniscope-calcium-imaging-using-convolutional-neural-networks/data/tmaze_2021-10-28T17_45_15.csv\"]\n",
    "    bonsai_paths = [\"/home/dmc/Desktop/kostas/direct-Behavior-prediction-from-miniscope-calcium-imaging-using-convolutional-neural-networks/data/tmaze_2021-10-28T18_13_23.csv\"]\n",
    "    if labels_type_new == False:\n",
    "        \n",
    "        # behavior_paths = [\"/home/dmc/Desktop/kostas/direct-Behavior-prediction-from-miniscope-calcium-imaging-using-convolutional-neural-networks/data/20211028_174510_animal2learnday11.h5\"]\n",
    "        behavior_paths = [\"/home/dmc/Desktop/kostas/direct-Behavior-prediction-from-miniscope-calcium-imaging-using-convolutional-neural-networks/data/20211028_181307_animal3learnday11.h5\"]\n",
    "        df_new_annotations, df_new_annotations_check = align_files_old_labels(bonsai_paths, behavior_paths, num_of_videos, merge_labels)\n",
    "    else:\n",
    "        h5_path = \"/home/dmc/Desktop/kostas/direct-Behavior-prediction-from-miniscope-calcium-imaging-using-convolutional-neural-networks/data/behavior_segmentation_arrowmaze.h5\"\n",
    "        \n",
    "        df_new_annotations, df_new_annotations_check = align_files_new_labels(bonsai_paths, num_of_videos, h5_path, multiple_videos)\n",
    "\n",
    "\n",
    "else:\n",
    "    \n",
    "    bonsai_paths = [\"/home/dmc/Desktop/kostas/direct-Behavior-prediction-from-miniscope-calcium-imaging-using-convolutional-neural-networks/data/tmaze_2021-10-25T18_48_49.csv\", \n",
    "                    \"/home/dmc/Desktop/kostas/direct-Behavior-prediction-from-miniscope-calcium-imaging-using-convolutional-neural-networks/data/tmaze_2021-10-26T14_29_27.csv\", \n",
    "                    \"/home/dmc/Desktop/kostas/direct-Behavior-prediction-from-miniscope-calcium-imaging-using-convolutional-neural-networks/data/tmaze_2021-10-27T16_50_53.csv\"]\n",
    "    \n",
    "    if labels_type_new == False:\n",
    "        \n",
    "        behavior_paths = [\"/home/dmc/Desktop/kostas/direct-Behavior-prediction-from-miniscope-calcium-imaging-using-convolutional-neural-networks/data/20211025_184906_animal3learnday8.h5\", \n",
    "                          \"/home/dmc/Desktop/kostas/direct-Behavior-prediction-from-miniscope-calcium-imaging-using-convolutional-neural-networks/data/20211026_142935_animal3learnday9.h5\", \n",
    "                          \"/home/dmc/Desktop/kostas/direct-Behavior-prediction-from-miniscope-calcium-imaging-using-convolutional-neural-networks/data/20211027_165052_animal3learnday10.h5\"]\n",
    "        \n",
    "        df_new_annotations, df_new_annotations_check = align_files_old_labels(bonsai_paths, behavior_paths, num_of_videos, merge_labels)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        h5_path = \"/home/dmc/Desktop/kostas/direct-Behavior-prediction-from-miniscope-calcium-imaging-using-convolutional-neural-networks/data/behavior_segmentation_arrowmaze.h5\"\n",
    "        df_new_annotations, df_new_annotations_check = align_files_new_labels(bonsai_paths, num_of_videos, h5_path, multiple_videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359a0d97-1a04-4e5e-9e80-9ef527f337e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_annotations_check #df_new_annotations_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13580686-d63d-4f91-91c5-d221461432f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "reordered_annotations = df_new_annotations_check.drop_duplicates()\n",
    "reordered_annotations = reordered_annotations.sort_values('state_id')\n",
    "reordered_annotations = reordered_annotations.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a878f70-22a5-4141-8821-ced96f043e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "reordered_annotations.at[0, 'state_name'] = 'Grooming'\n",
    "reordered_annotations.at[1, 'state_name'] = 'Frozen'\n",
    "reordered_annotations.at[2, 'state_name'] = 'Not moving'\n",
    "reordered_annotations.at[3, 'state_name'] = 'Moving'\n",
    "reordered_annotations.at[4, 'state_name'] = 'Right turn'\n",
    "reordered_annotations.at[5, 'state_name'] = 'Left turn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3abec5-8813-4e21-91d8-ff276825f296",
   "metadata": {},
   "outputs": [],
   "source": [
    "reordered_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61afea61-c474-4444-bb40-3517891a1a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedc503a-c136-474c-8367-b3afa34f569d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Step 7: Aligning Behavior with Calcium video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42b2a59-60ad-4d65-82a6-2fc7d27c7d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_annotations = df_new_annotations.reset_index(drop=True)\n",
    "df_new_annotations_unique = reordered_annotations['state_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d6624a-6eb4-4c79-87b5-deb0e9d38b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_annotations_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cac562-4144-43ff-aa4a-6ad97fb1715c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c5c54a-f53c-41a9-831d-df432b0365ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_annotations_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bb66a0-f68b-4bb7-8837-d83ab24f1ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels_number = len()'all' # can be also 3 (merged) or 6 (new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb33fe17-a466-48f9-b9fc-8b173667e983",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_labels = len(reordered_annotations)\n",
    "%store no_of_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad497ef-c726-46fb-ae42-2ddc81ca5eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea69a99-fed1-4aae-b0b6-aab5325519cd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Step 8: Check the balance of the behaviour labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e8f21f-729a-423c-b4d3-2db9266e190e",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"/home/dmc/Desktop/kostas/direct-Behavior-prediction-from-miniscope-calcium-imaging-using-convolutional-neural-networks/src/V3/\"+str(output_dir)+\"/balance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134adbac-99c5-4a11-8881-2b568085a71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check balance\n",
    "total_counts = 0\n",
    "for i in range(len(df_new_annotations_unique)):\n",
    "    class_counts = pd.value_counts(df_new_annotations_check['state_id'])\n",
    "    total_counts = total_counts + class_counts[i]\n",
    "# calculate the percentage of each class in the dataset\n",
    "class_percents = pd.value_counts(df_new_annotations, normalize=True) * 100\n",
    "print(class_percents,\"\\n\",class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ebdbb4-3fcb-4bbf-bfc1-037c4c059869",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = reordered_annotations['state_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd5bb50-b36b-4ad0-8865-08ef2c80f33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01334238-8426-46ae-a6d1-9a21382b7b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if labels_type_new == True:\n",
    "    class_counts, total_counts = check_class_imbalance_new(df_new_annotations, \n",
    "                                                           experiment_ID, \n",
    "                                                           save_dir, \n",
    "                                                           df_new_annotations_unique, \n",
    "                                                           df_new_annotations_check, \n",
    "                                                           no_of_labels, \n",
    "                                                           data_file,\n",
    "                                                           label_names)\n",
    "\n",
    "elif merge_labels == True:\n",
    "    names_of_labels = 'Main Corr', 'Left Corr', 'Right Corr'\n",
    "    class_counts, total_counts = check_class_imbalance_old_merged(df_new_annotations, \n",
    "                                                                  experiment_ID, \n",
    "                                                                  save_dir, \n",
    "                                                                  df_new_annotations_unique, \n",
    "                                                                  df_new_annotations_check, \n",
    "                                                                  no_of_labels, \n",
    "                                                                  names_of_labels, \n",
    "                                                                  data_file,\n",
    "                                                                  label_names)\n",
    "\n",
    "else:\n",
    "    class_counts, total_counts = check_class_imbalance_old(df_new_annotations, \n",
    "                                                           experiment_ID, \n",
    "                                                           save_dir, \n",
    "                                                           df_new_annotations_unique, \n",
    "                                                           df_new_annotations_check, \n",
    "                                                           no_of_labels, \n",
    "                                                           data_file,\n",
    "                                                           label_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bab359-ef5c-483e-9060-aec8dc9b5f8c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Step 9: Data Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921ff502-bfdc-4892-b0fe-471738bc087a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the Images and Labels have the same length\n",
    "labels = df_new_annotations\n",
    "if len(labels) == len(images):\n",
    "    print(\"Labels and Images have the same length:\", len(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ddf3ba-4189-4ef5-b77e-60cd885f6be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_first_frames(images, labels, data_file)\n",
    "plot_random_frames(images, labels, data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f71c8a-c332-43c1-8151-6debc36de4ea",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Step 10: Create the input shape of the data before model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f324ada6-75a2-444d-a769-69c56185ac40",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a51d630-0706-495f-a76c-d93b56a0048a",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = model_preprocessing(images, \n",
    "                                     labels, \n",
    "                                     df_new_annotations_unique,\n",
    "                                     num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56f2d19-982f-41d4-b1b9-e8d494609c5e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 11: Training the BPNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426e0d30-44f0-4080-bea3-9b88d97ce0c9",
   "metadata": {},
   "source": [
    "#### Set the Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecef43dd-3912-44e3-b5f0-23c6ea3cf521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model type\n",
    "is_basic_BPNN = False # True means WITHOUT time dimension. False means WITH time dimension\n",
    "\n",
    "if is_basic_BPNN == True:\n",
    "    channel_dimension = 1\n",
    "else:\n",
    "    channel_dimension = 7 #3 or 5 if you're including the time dimension\n",
    "\n",
    "input_shape = (img_height, \n",
    "               img_width, \n",
    "               channel_dimension)\n",
    "\n",
    "# set number of folds\n",
    "num_folds = 2 # 10\n",
    "\n",
    "# number of epochs\n",
    "epochs = 1\n",
    "\n",
    "# define if shuffle data\n",
    "shuffle = False\n",
    "\n",
    "# names of unique behaviours\n",
    "df_new_annotations_names = label_names.unique()\n",
    "\n",
    "# number of unique behaviour labels\n",
    "no_of_behaviors = len(label_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2117342d-98ae-488e-941c-4f9c529566ff",
   "metadata": {},
   "source": [
    "#### Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8ea918-fa61-4a64-bc6d-f3bb48e4fe19",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_basic_BPNN == True:\n",
    "    print(\"Running \"+data_file_file+\" on the BPNN, with 5-Fold CV, \"+epochs+\" Epochs, and Early Stopping\")\n",
    "    train_loss_all, val_loss_all, train_acc_all, val_acc_all, average_score_list, conf_matrices, f1_score_val_list, train_labels_names = run(is_basic_BPNN,\n",
    "        label_names,\n",
    "        labels_type_new, \n",
    "        shuffle,\n",
    "        images,\n",
    "        labels,\n",
    "        num_folds,\n",
    "        #shuffled_labels,\n",
    "        input_shape,\n",
    "        num_classes,\n",
    "        name,\n",
    "        epochs,\n",
    "        no_of_behaviors,\n",
    "        df_new_annotations,\n",
    "        df_new_annotations_unique,\n",
    "        df_new_annotations_check,\n",
    "        output_dir,\n",
    "        experiment_ID)\n",
    "else:\n",
    "    print(\"Running \"+str(data_file)+\" on the BPNNt, with 5-Fold CV, \"+str(epochs)+\" Epochs, and Early Stopping\")\n",
    "    train_loss_all, val_loss_all, train_acc_all, val_acc_all, average_score_list, conf_matrices, f1_score_val_list, train_labels_names = run(is_basic_BPNN,\n",
    "        label_names,\n",
    "        labels_type_new, \n",
    "        shuffle,\n",
    "        images,\n",
    "        labels,\n",
    "        num_folds,\n",
    "        #shuffled_labels,\n",
    "        input_shape,\n",
    "        num_classes,\n",
    "        name,\n",
    "        epochs,\n",
    "        no_of_behaviors,\n",
    "        df_new_annotations,\n",
    "        df_new_annotations_unique,\n",
    "        df_new_annotations_check,\n",
    "        output_dir,\n",
    "        experiment_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103fbbfa-9199-4514-aa80-e1fd57127ed8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b95f5e5-933a-4cd4-b580-98f6e372b88c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 12: Saving Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149be8f4-c0de-4c45-9336-ebc6d4a48b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_names\n",
    "%store train_labels_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcab662-8bad-401b-bd4a-b7cf9223d34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_mean = np.mean(f1_score_val_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a952f4ee-8319-4ec2-aa38-e63e7112f363",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d7b734-83bb-4c94-bc35-5572c19e3fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot the f1 score per fold to see how the performance falls with time. \n",
    "\n",
    "\n",
    "# labels = ['Fold 1', 'Fold 2', 'Fold 3', 'Fold 4', 'Fold 5']\n",
    "\n",
    "# # Choose a color palette from seaborn's built-in palettes\n",
    "# color_palette = 'Set2'\n",
    "\n",
    "\n",
    "# sns.barplot(x=labels, y=f1_score_val_list, palette=color_palette)\n",
    "# plt.xlabel('Fold')\n",
    "# plt.ylabel('F1 Score')\n",
    "# plt.title('F1 Score for Each Fold')\n",
    "\n",
    "# # Specify the folder path where you want to save the plot\n",
    "# save_folder = '/home/dmc/Desktop/kostas/direct-Behavior-prediction-from-miniscope-calcium-imaging-using-convolutional-neural-networks/src/V3'\n",
    "\n",
    "# # Save the plot in the specified folderf1\n",
    "# plt.savefig(save_folder + 'f1_scores_per_fold.svg', dpi=300, bbox_inches='tight')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3271fb9c-90ae-4658-970a-ed7165b9f562",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_val_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059bbe4f-4fb4-43b5-91fa-d9e42312c528",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6db1ba-683c-4a6f-a331-69859ab04051",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_acc_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8749026-5558-4f25-b899-8e6c2159d4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8be933-6176-41cb-bee8-505dc8917417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #plot the mean confusion matrix\n",
    "# mean_cm = np.mean(conf_matrices, axis=0)\n",
    "# # train_labels_names = ['moving', 'rightTurn', 'immobile', 'grooming', 'still', 'leftTurn']\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# sns.heatmap(mean_cm, annot=True, cmap='Blues', fmt='g')\n",
    "# plt.title('Mean Confusion Matrix - K-Fold Cross Validation')\n",
    "# plt.xlabel('Predicted Labels')\n",
    "# plt.xticks(np.arange(len(train_labels_names)) + 0.5, train_labels_names, rotation=90, fontsize=6)\n",
    "# plt.ylabel('True Labels')\n",
    "# plt.yticks(np.arange(len(train_labels_names)) + 0.5, train_labels_names, rotation=0, fontsize=6)\n",
    "# plt.savefig(\"mean_CM\"+str(experiment_ID)+'.svg', bbox_inches='tight', dpi=300)\n",
    "# plt.show()\n",
    "\n",
    "# %store conf_matrices\n",
    "# %store train_labels_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2973fa7-49ce-4af8-b453-68fdb759add1",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"/home/dmc/Desktop/kostas/direct-Behavior-prediction-from-miniscope-calcium-imaging-using-convolutional-neural-networks/src/V3/\"+str(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86366d0b-972b-4bff-80e1-2170af68f646",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store train_loss_all\n",
    "%store val_loss_all\n",
    "%store train_acc_all\n",
    "%store val_acc_all\n",
    "%store f1_score_val_list\n",
    "%store f1_score_mean\n",
    "%store epochs\n",
    "%store total_accuracy_score\n",
    "%store num_folds\n",
    "%store conf_matrices\n",
    "%store num_folds\n",
    "%store no_of_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4035f852-6d5a-4a87-89dc-e6a1fde84c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %store model\n",
    "# %store history\n",
    "%store name\n",
    "%store comment\n",
    "%store save_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c060ff77-fc47-4791-9f63-3dfb681bfcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(val_acc_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e8b78b-df13-488b-9540-0c676484b112",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Snippets for running the model without 5-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871ed3d0-e03e-40c0-81c3-413aeb397382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split the data into training and validation sets\n",
    "split_index = int(0.2 * len(images))  # Index to split data\n",
    "\n",
    "# images = np.concatenate([images, images, images], axis=-1)\n",
    "\n",
    "val_images, train_images = images[:split_index], images[split_index:]\n",
    "val_labels, train_labels = labels[:split_index], labels[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c0888d-2546-4001-be62-9fb3c38ff632",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, val_images, train_labels, val_labels, num_classes = model_preprocessing(train_images, val_images, train_labels, val_labels, df_new_annotations_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51c845a-ca63-489f-a54f-e3422eb1bbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot the first 5 random images\n",
    "plot_first_frames(train_images, train_labels, vmin, vmax)\n",
    "plot_first_frames(val_images, val_labels, vmin, vmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33d2a18-cee9-4f83-8e80-e89e1ef699c8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Reflect on the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e43045-42d0-4f41-8add-ac3e5b08dd82",
   "metadata": {},
   "source": [
    "High bias: If the training accuracy is low, it suggests that the model is underfitting the training data, i.e., it is not complex enough to capture the patterns in the data. In this case, you may need to increase the model's complexity by adding more layers or neurons, or by using a more complex architecture.\n",
    "\n",
    "High variance: If the training accuracy is high but the validation accuracy is low, it suggests that the model is overfitting the training data, i.e., it is memorizing the training data instead of generalizing to new data. In this case, you may need to use regularization techniques like dropout or L2 regularization, or use early stopping to prevent the model from overfitting.\n",
    "\n",
    "Good fit: If the training accuracy and validation accuracy are both high and close to each other, it suggests that the model is neither underfitting nor overfitting the data, i.e., it is generalizing well to new data.\n",
    "\n",
    "Plateauing: If the validation accuracy is no longer increasing as the training set size or epochs increase, it suggests that the model has reached its capacity and adding more data or epochs is unlikely to improve its performance.\n",
    "\n",
    "In general, a model accuracy curve can help you diagnose issues with your model and guide you in selecting appropriate strategies to improve its performance. It can also give you an idea of how much training data or how many epochs you need to achieve good performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c7f0d1-8e62-4c31-8d65-d7e144301753",
   "metadata": {},
   "source": [
    "1. Insufficient data? One calcium video of 24186 frames and with 349x374 dimensions.\n",
    "2. Model architecture not appropriate. Try increasing the number of layers or filters, or adding more complex layers like BatchNormalization, Dropout, or Conv2DTranspose.\n",
    "3. Incorrect data preprocessing\n",
    "4. Incorrect hyperparameters\n",
    "5. Class Imbalance (do oversampling, or undersampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d604a71-bd99-4fd6-a5c8-9418d60bfb67",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Reusable snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b0684b-764a-418c-a033-ff5fe9ce130b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load calcium video from local environment\n",
    "# with h5py.File('path', 'r') as f:\n",
    "#     video_data = np.array(f['analysis/recording_20211016_163921-PP-BP-MC/data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b34b51-5633-42e3-8399-6842489b6bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading locally\n",
    "# with h5py.File('/Users/konstantinoskalaitzidis/Developer/dmc/thesis_data/20211016_163921_animal1learnday1.h5', 'r') as f:\n",
    "#     print(list(f.keys()))\n",
    "#     behavior_data = np.array(f['per_frame'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22276208-f41e-4589-9672-50de6e38d4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save the model architecture to a JSON file\n",
    "# with open('model_architecture.json', 'w') as f:\n",
    "#     f.write(model.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dc765c-32f7-4a47-bc62-518a371b26b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load the model architecture from the JSON file\n",
    "# with open('model_architecture.json', 'r') as f:\n",
    "#     json_string = f.read()\n",
    "\n",
    "# model_json = model_from_json(json_string)\n",
    "\n",
    "# # print the loaded model summary\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c1dfa2-4562-4f80-97ab-cef413ef7689",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Other"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1cc62e-96d3-4a65-aaff-a4f869cff76b",
   "metadata": {},
   "source": [
    "Helpful source: https://keras.io/examples/vision/video_classification/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f96f31b-c607-4a50-bb5f-9ef8fecdd3e3",
   "metadata": {},
   "source": [
    "The number of frames may differ from video to video.\n",
    "The frame rate may also differ from video to video but it should be 20fps for all. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dcdb7b-7022-485c-9c96-591b3711feab",
   "metadata": {},
   "source": [
    "The duration of each frame depends on the frame rate of the video. If a video has a frame rate of 25 fps, then each frame will have a duration of 1/25th of a second, or approximately 0.04 seconds. The calcium videos use 20fps, while the behavioral recordings are at 60fps. Alignment of these videos will follow shortly. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
<<<<<<< HEAD:src/V3/BPNN_V3.ipynb
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
=======
>>>>>>> revamp-release:bpnn/bpnn.ipynb
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
